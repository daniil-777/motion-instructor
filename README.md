# MR Motion Instructor

# Abstract
Learning complex movements can be a time-consuming process, but it is necessary for the mastery of activities like Karate Kata, Yoga and dance choreographies. It is important to have a teacher in person to demonstrate the correct pose sequences step by step and correct errors in the studentâ€™s body postures.
In-person sessions can be impractical due to epidemics or travel distance, while videos make it hard to see the 3D postures of the teacher and of the students. As an alternative, we propose the teaching of poses in Augmented Reality (AR) with a virtual teacher and 3D avatars.

## Open source usage
This work is based on [this repository](https://github.com/curiosity-inc/azure-kinect-dk-unity) (MIT License), which is an example C# wrapper showing how to use Azure Kinect DK in Unity. In particular, we used code from [this fork](https://github.com/Aviscii/azure-kinect-dk-unity). We upgraded the Azure Kinect Body Tracking SDK and Azure Kinect SDK.
As alternative to pose estimation with Kinect we used [Lightweight human pose estimation](https://github.com/Daniil-Osokin/lightweight-human-pose-estimation-3d-demo.pytorch) (Apache-2.0 License). 

## Environment
As of July 5, 2020, this repo has been tested under the following environment.
- Windows 10 Education N (Version 10.0.18363) with Media Feature Pack (important)
- [Azure Kinect Sensor SDK](https://docs.microsoft.com/en-us/azure/Kinect-dk/sensor-sdk-download) v1.4.0
  - test by running `k4aviewer.exe` in `C:\Program Files\Azure Kinect SDK v1.4.0\tools`
- [Azure Kinect Body Tracking SDK](https://docs.microsoft.com/en-us/azure/Kinect-dk/body-sdk-download) v0.9.0
  - test by running `k4abt_simple_3d_viewer.exe` in `C:\Program Files\Azure Kinect Body Tracking SDK\tools`
- Unity 2019.4.13f1 // newest Unity 2019 LTS version at the moment
- CUDA v10.0
  - try `nvcc  --version` (ONNX runtime officially only supports up to CUDA 10.0) 
- cudnn v7.5.1.10 
  - check `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\include\cudnn.h`
- [all other tools required to develop on the Hololens](https://docs.microsoft.com/en-us/windows/mixed-reality/install-the-tools)

## Get Started
1. Clone the poseteacher2 repo.
2. (optional but recommended) For setting up real-time pose estimation. Otherwise a motion from a file will be used instead. 
   1. With Azure Kinect: Copy the following files from Azure Kinect Sensor SDK or Azure Kinect Body Tracking SDK to the folder unity in the poseteacher2 repo:
    
      - k4a.dll (Body Tracking SDK or Kinect SDK)
      - k4abt.dll (Body Tracking SDK)
      - onnxruntime.dll (Body Tracking SDK)
      - depthengine_2_0.dll (Body Tracking SDK or Kinect SDK)
      - dnn_model_2_0.onnx (Body Tracking SDK)
      - make sure that cudnn is properly installed and in the path. Otherwise it might be necessary to add the cudnn files to the unity folder as well. 

   2. Without Azure Kinect or a GPU: Clone [this repository](https://github.com/Daniil-Osokin/lightweight-human-pose-estimation-3d-demo.pytorch) and copy `demo_ws.py` into its main folder. Then install the required packages according to the repo and run `demo_ws.py`. Beware that Pytorch still has issues with Python 3.8, so we recommend using Python 3.7.
3. Open the `unity` folder as a Unity project, with `Universal Windows Platform` as the build platform. It might take a while to fetch all packages.
4. Open `Assets/PoseteacherScene`.
5. When prompted to import TextMesh Pro, select `Import TMP Essential Resources`. You will need to reopen the scene to fix visual glitches.
6. Connect to the Hololens with [Holographic Remoting](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/Tools/HolographicRemoting.html#connecting-to-the-hololens-with-wi-fi) (using the `Windows XR Plugin Remoting` in Unity).
7. Click play inside the Unity editor.

### Notes
- Most of the application logic is inside of the `PoseteacherMain.cs` script which is attached to the `Main` game object.
- If updating from an old version of the project, be sure to delete the `Library` folder generated by Unity, so that packages are handled correctly. 
- The project uses MRTK 2.5.1 from the Unity Package Manager, not imported into the `Assets` folder: 
   - MRTK assets have to be searched in inside `Packages` in the editor.
   - The only MRTK files in `Assets` should be in folders `MixedRealityToolkit.Generated` and `MRTK/Shaders`. 
   - Only exception is the `Samples/Mixed Reality Toolkit Examples` if [MRTK examples are imported](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/usingupm.html#using-mixed-reality-toolkit-examples)
   - If there are other MRTK folders, they are from an old version of the project (or were imported manually) and [should be removed like when updating](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/Updating.html). 
     Remeber to delete the `Library` folder after doing this.
- We use the newer XR SDK pipeline instead of the Legacy XR pipeline (which is depreciated)

## How to use
- It is recommended to use the UI to navigate in the application.
- ~~For debugging we added the following keyboard shortcuts:~~ (currently unavailable)
   - press X for normal operation
   - press Y for recording
   - press Z for playback
   - press L to load playback
   - press R to reset/delete saved playback

## Refactor TODOs

Refactored project organization:

The `PoseteacherMain.cs` script shoud be divided onto relevant objects in the scene:
   Attach a separate script on each avatar object in scene.
 - Pose Interface script (attached to empty object? or called from main) functionalities:
   - [ ] Get stream of poses from Azure Kinect BT SDK (or websocket)
   - [ ] Get stream of poses from a `.json` file
   - [ ] Write stream of poses to file.
   - [ ] Start/Pause/Restart/Speed for movements from files
 - Main script (attached to empty object) functionalities:
   - [ ] (optional) change so that avatar containers are added to scene when needed.
   